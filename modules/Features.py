import os
import numpy as np
import opensmile
import pandas as pd
import librosa as lb
from torch import Tensor, reshape
from pyannote.audio.pipelines import VoiceActivityDetection, OverlappedSpeechDetection

MID_LEVEL_NAMES = [
    'pcm_RMSenergy_sma_stddev', 'pcm_RMSenergy_sma_skewness', 'pcm_RMSenergy_sma_kurtosis',
    'pcm_RMSenergy_sma_meanSegLen',
    'pcm_zcr_sma_stddev', 'pcm_zcr_sma_skewness', 'pcm_zcr_sma_kurtosis', 'pcm_zcr_sma_meanSegLen',
    'pcm_RMSenergy_sma_de_stddev', 'pcm_RMSenergy_sma_de_skewness', 'pcm_RMSenergy_sma_de_kurtosis',
    'pcm_RMSenergy_sma_de_meanSegLen',
    'pcm_zcr_sma_de_stddev', 'pcm_zcr_sma_de_skewness', 'pcm_zcr_sma_de_kurtosis', 'pcm_zcr_sma_de_meanSegLen',
    'pcm_fftMag_spectralRollOff25.0_sma_stddev', 'pcm_fftMag_spectralRollOff25.0_sma_skewness',
    'pcm_fftMag_spectralRollOff25.0_sma_kurtosis', 'pcm_fftMag_spectralRollOff25.0_sma_meanSegLen',
    'pcm_fftMag_spectralRollOff50.0_sma_stddev', 'pcm_fftMag_spectralRollOff50.0_sma_skewness',
    'pcm_fftMag_spectralRollOff50.0_sma_kurtosis', 'pcm_fftMag_spectralRollOff50.0_sma_meanSegLen',
    'pcm_fftMag_spectralRollOff75.0_sma_stddev', 'pcm_fftMag_spectralRollOff75.0_sma_skewness',
    'pcm_fftMag_spectralRollOff75.0_sma_kurtosis', 'pcm_fftMag_spectralRollOff75.0_sma_meanSegLen',
    'pcm_fftMag_spectralRollOff90.0_sma_stddev', 'pcm_fftMag_spectralRollOff90.0_sma_skewness',
    'pcm_fftMag_spectralRollOff90.0_sma_kurtosis', 'pcm_fftMag_spectralRollOff90.0_sma_meanSegLen',
    'pcm_fftMag_spectralFlux_sma_stddev', 'pcm_fftMag_spectralFlux_sma_skewness',
    'pcm_fftMag_spectralFlux_sma_kurtosis', 'pcm_fftMag_spectralFlux_sma_meanSegLen',
    'pcm_fftMag_spectralCentroid_sma_stddev', 'pcm_fftMag_spectralCentroid_sma_skewness',
    'pcm_fftMag_spectralCentroid_sma_kurtosis', 'pcm_fftMag_spectralCentroid_sma_meanSegLen',
    'pcm_fftMag_spectralEntropy_sma_stddev', 'pcm_fftMag_spectralEntropy_sma_skewness',
    'pcm_fftMag_spectralEntropy_sma_kurtosis', 'pcm_fftMag_spectralEntropy_sma_meanSegLen',
    'pcm_fftMag_spectralVariance_sma_stddev', 'pcm_fftMag_spectralVariance_sma_skewness',
    'pcm_fftMag_spectralVariance_sma_kurtosis', 'pcm_fftMag_spectralVariance_sma_meanSegLen',
    'pcm_fftMag_spectralSkewness_sma_stddev', 'pcm_fftMag_spectralSkewness_sma_skewness',
    'pcm_fftMag_spectralSkewness_sma_kurtosis', 'pcm_fftMag_spectralSkewness_sma_meanSegLen',
    'pcm_fftMag_spectralKurtosis_sma_stddev', 'pcm_fftMag_spectralKurtosis_sma_skewness',
    'pcm_fftMag_spectralKurtosis_sma_kurtosis', 'pcm_fftMag_spectralKurtosis_sma_meanSegLen',
    'pcm_fftMag_spectralSlope_sma_stddev', 'pcm_fftMag_spectralSlope_sma_skewness',
    'pcm_fftMag_spectralSlope_sma_kurtosis', 'pcm_fftMag_spectralSlope_sma_meanSegLen',
    'pcm_fftMag_psySharpness_sma_stddev', 'pcm_fftMag_psySharpness_sma_skewness',
    'pcm_fftMag_psySharpness_sma_kurtosis', 'pcm_fftMag_psySharpness_sma_meanSegLen',
    'pcm_fftMag_spectralHarmonicity_sma_stddev', 'pcm_fftMag_spectralHarmonicity_sma_skewness',
    'pcm_fftMag_spectralHarmonicity_sma_kurtosis', 'pcm_fftMag_spectralHarmonicity_sma_meanSegLen',
    'mfcc_sma[1]_stddev', 'mfcc_sma[1]_skewness', 'mfcc_sma[1]_kurtosis', 'mfcc_sma[1]_meanSegLen',
    'mfcc_sma[2]_stddev', 'mfcc_sma[2]_skewness', 'mfcc_sma[2]_kurtosis', 'mfcc_sma[2]_meanSegLen',
    'mfcc_sma[3]_stddev', 'mfcc_sma[3]_skewness', 'mfcc_sma[3]_kurtosis', 'mfcc_sma[3]_meanSegLen',
    'mfcc_sma[4]_stddev', 'mfcc_sma[4]_skewness', 'mfcc_sma[4]_kurtosis', 'mfcc_sma[4]_meanSegLen',
    'mfcc_sma[5]_stddev', 'mfcc_sma[5]_skewness', 'mfcc_sma[5]_kurtosis', 'mfcc_sma[5]_meanSegLen',
    'mfcc_sma[6]_stddev', 'mfcc_sma[6]_skewness', 'mfcc_sma[6]_kurtosis', 'mfcc_sma[6]_meanSegLen',
    'mfcc_sma[7]_stddev', 'mfcc_sma[7]_skewness', 'mfcc_sma[7]_kurtosis', 'mfcc_sma[7]_meanSegLen',
    'mfcc_sma[8]_stddev', 'mfcc_sma[8]_skewness', 'mfcc_sma[8]_kurtosis', 'mfcc_sma[8]_meanSegLen',
    'mfcc_sma[9]_stddev', 'mfcc_sma[9]_skewness', 'mfcc_sma[9]_kurtosis', 'mfcc_sma[9]_meanSegLen',
    'mfcc_sma[10]_stddev', 'mfcc_sma[10]_skewness', 'mfcc_sma[10]_kurtosis', 'mfcc_sma[10]_meanSegLen',
    'mfcc_sma[11]_stddev', 'mfcc_sma[11]_skewness', 'mfcc_sma[11]_kurtosis', 'mfcc_sma[11]_meanSegLen',
    'mfcc_sma[12]_stddev', 'mfcc_sma[12]_skewness', 'mfcc_sma[12]_kurtosis', 'mfcc_sma[12]_meanSegLen',
    'mfcc_sma[13]_stddev', 'mfcc_sma[13]_skewness', 'mfcc_sma[13]_kurtosis', 'mfcc_sma[13]_meanSegLen',
    'mfcc_sma[14]_stddev', 'mfcc_sma[14]_skewness', 'mfcc_sma[14]_kurtosis', 'mfcc_sma[14]_meanSegLen',
    'jitterLocal_sma_amean', 'jitterLocal_sma_stddev', 'jitterLocal_sma_skewness', 'jitterLocal_sma_kurtosis',
    'jitterDDP_sma_amean', 'jitterDDP_sma_stddev', 'jitterDDP_sma_skewness', 'jitterDDP_sma_kurtosis',
    'shimmerLocal_sma_amean', 'shimmerLocal_sma_stddev', 'shimmerLocal_sma_skewness', 'shimmerLocal_sma_kurtosis',
    'pcm_RMSenergy_sma_amean',
    'pcm_zcr_sma_amean',
    'pcm_fftMag_spectralRollOff25.0_sma_amean',
    'pcm_fftMag_spectralRollOff50.0_sma_amean',
    'pcm_fftMag_spectralRollOff75.0_sma_amean',
    'pcm_fftMag_spectralRollOff90.0_sma_amean',
    'pcm_fftMag_spectralFlux_sma_amean',
    'pcm_fftMag_spectralCentroid_sma_amean',
    'pcm_fftMag_spectralEntropy_sma_amean',
    'pcm_fftMag_spectralVariance_sma_amean',
    'pcm_fftMag_spectralSkewness_sma_amean',
    'pcm_fftMag_spectralKurtosis_sma_amean',
    'pcm_fftMag_spectralSlope_sma_amean',
    'pcm_fftMag_psySharpness_sma_amean',
    'pcm_fftMag_spectralHarmonicity_sma_amean',
    'mfcc_sma[1]_amean',
    'mfcc_sma[2]_amean',
    'mfcc_sma[3]_amean',
    'mfcc_sma[4]_amean',
    'mfcc_sma[5]_amean',
    'mfcc_sma[6]_amean',
    'mfcc_sma[7]_amean',
    'mfcc_sma[8]_amean',
    'mfcc_sma[9]_amean',
    'mfcc_sma[10]_amean',
    'mfcc_sma[11]_amean',
    'mfcc_sma[12]_amean',
    'mfcc_sma[13]_amean',
    'mfcc_sma[14]_amean'
]


class AudioFeature:

    def __init__(self) -> None:
        pass

    def __call__(self,
                 turn_taking_df,
                 prosody_df):
        turn_taking = turn_taking_df.drop(
            columns=['Person_vad', 'Person_ovd', 'File_ovd', 'Start_vad',
                     'Start_ovd', 'End_vad', 'End_ovd', 'Middle_vad',
                     'Middle_ovd']
        )
        turn_taking.rename(columns={'File_vad': 'File'},
                           inplace=True)
        prosody = prosody_df.loc[:, MID_LEVEL_NAMES]
        # prosody['File'] = prosody_df.loc[:, 'File']
        prosody.set_index(pd.RangeIndex(start=0, stop=prosody.shape[0]),
                          inplace=True)
        turn_taking.set_index(pd.RangeIndex(start=0, stop=turn_taking.shape[0]),
                              inplace=True)

        dataset = turn_taking.join(prosody)
        return dataset.set_index('File')

        # return dataset.reindex(sorted(dataset.columns), axis=1)


class TurnTaking:

    def __init__(self,
                 vad_onset=0.648,
                 vad_offset=0.577,
                 vad_min_dur_on=0.181,
                 vad_min_dur_off=0.037,
                 ovd_onset=0.448,
                 ovd_offset=0.362,
                 ovd_min_dur_on=0.116,
                 ovd_min_dur_off=0.187):
        self.vad_onset = vad_onset
        self.vad_offset = vad_offset
        self.vad_min_dur_on = vad_min_dur_on
        self.vad_min_dur_off = vad_min_dur_off
        self.ovd_onset = ovd_onset
        self.ovd_offset = ovd_offset
        self.ovd_min_dur_on = ovd_min_dur_on
        self.ovd_min_dur_off = ovd_min_dur_off

    def __make_df(self,
                  path: str,
                  iterable_track: iter) -> pd.DataFrame:
        """
        Loop through the iterable return from pyAnnote functions and
        build a dataframe.\n
        Dataframe is filled with Nan if iterable is empty
        :param path: Absolute path of the audio file processed
        :param iterable_track: Iterable return by pyAnnote functions
        :return: Pandas Dataframe
        """
        file_name = os.path.splitext(os.path.basename(path))[0]

        data = {}

        # Loop the iterable and store data in a dictionary
        for tracks, person, _ in iterable_track.itertracks(yield_label=True):
            data['File'] = file_name
            data['Person'] = person
            data['Start'] = tracks.start
            data['End'] = tracks.end
            data['Duration'] = tracks.duration
            data['Middle'] = tracks.middle

        df = pd.DataFrame([data])  # Make DataFrame from dict

        if df.empty:  # If iterable is empty fill dict with Nan
            data = {'Person': np.nan,
                    'Start': np.nan,
                    'End': np.nan,
                    'Duration': np.nan,
                    'Middle': np.nan
                    }
            df = pd.DataFrame([data])

        df['File'] = file_name

        return df

    def __voice_activity(self, audio_in_memory):
        pipeline = VoiceActivityDetection(segmentation='pyannote/segmentation')
        hyper_parameters = {
            "onset": self.vad_onset,
            "offset": self.vad_offset,
            "min_duration_on": self.vad_min_dur_on,
            "min_duration_off": self.vad_min_dur_off
        }

        pipeline.instantiate(hyper_parameters)

        return pipeline(audio_in_memory)

    def __overlapped_speech(self, audio_in_memory):
        pipeline = OverlappedSpeechDetection(segmentation='pyannote/segmentation')
        hyper_parameters = {
            "onset": self.ovd_onset,
            "offset": self.ovd_offset,
            "min_duration_on": self.ovd_min_dur_on,
            "min_duration_off": self.ovd_min_dur_off
        }

        pipeline.instantiate(hyper_parameters)

        return pipeline(audio_in_memory)

    def __postprocess_turn_taking(self, df_vad, df_ovd, frame_length, sr):
        df_vad['Silence'] = (frame_length / sr) - df_vad['Duration']
        df_vad['Speech ratio'] = df_vad['Duration'] / df_vad['Silence']

        dataset = df_vad.join(df_ovd,
                              lsuffix='_vad',
                              rsuffix='_ovd')
        return dataset

    def __call__(self,
                 path: str,
                 sr: int = 44100,
                 frame_length: int = 220500,
                 hop_length: int = 176400) -> pd.DataFrame:
        """
        When calling the class it starts the processing of the audio
        and compute the amount of Voice Activity and Overlapped Speech.\n
        Default values are for processing audio at 44,1kHz on
        frames of 5s with 1s of overlap\n
        :param path: Absolute path of the audio file processed
        :param sr: Sampling frequency at which load and process the file
        :param frame_length: Length of the frame in frames
        :param hop_length: Starting position of the next frame,
        from the previous start
        :return: Pandas DataFrame with files names as index and start,
        end, middle point and duration of Voice Activity and Overlapped Speech
        """

        df_vad = pd.DataFrame()
        df_ovd = pd.DataFrame()

        audio, sr = lb.load(path,
                            sr)
        for i in range(0, len(audio), hop_length):
            waveform = reshape(Tensor(audio[i:i + frame_length]), (1, -1))
            audio_in_memory = {"waveform": waveform, "sample_rate": sr}
            vad = self.__voice_activity(audio_in_memory=audio_in_memory)
            ovd = self.__overlapped_speech(audio_in_memory=audio_in_memory)

            df_vad = pd.concat([df_vad, self.__make_df(path, vad)])
            df_ovd = pd.concat([df_ovd, self.__make_df(path, ovd)])

        stop = df_vad.shape[0]
        df_vad.set_index(pd.RangeIndex(start=0, stop=stop), inplace=True)
        df_ovd.set_index(pd.RangeIndex(start=0, stop=stop), inplace=True)

        out_df = self.__postprocess_turn_taking(
            df_vad=df_vad,
            df_ovd=df_ovd,
            frame_length=frame_length,
            sr=sr)

        return out_df


class Prosody:  # TO-DO: implementare possibilità di scegliere le feature da un file di configurazione
    def __init__(self):
        self.smile = opensmile.Smile(
            feature_set=opensmile.FeatureSet.ComParE_2016,
            feature_level=opensmile.FeatureLevel.Functionals,

        )

    def __call__(self,
                 path,
                 sr,
                 frame_length,
                 hop_factor):
        file_name = os.path.splitext(os.path.basename(path))[0]
        audio, sr = lb.load(path,
                            sr=sr)
        audio_feat = pd.DataFrame()
        for i in range(0, len(audio), hop_factor):
            signal = audio[i:i + frame_length]
            if len(signal) < frame_length:
                signal = np.pad(signal, (0, frame_length - len(signal)))
            df = self.smile.process_signal(
                signal=signal,
                sampling_rate=sr
            )
            df['File'] = file_name
            audio_feat = pd.concat([audio_feat, df])
        return audio_feat
